

# 梯度下降

梯度好文 https://blog.csdn.net/iqdutao/article/details/107174240

学习率/步长

## 批量梯度下降算法 BGD
全量梯度下降不能进行在线模型参数更新
更新参数时使用全部样本来进行更新，每次前进都是正确的方向，下降过程基本是直线


## 随机梯度下降算法 SGD

每次更新系数只随机抽取一个样本进行计算，每次更新不一定是正确的方向。随机性比较大，下降过程比较曲折

## 小批量梯度下降 MBGD
每次使用一部分样本，一般小于256，这样比SGD稳定，比BGD快

## GD 下降的困难

学习速率很难选择到一个合理的，太小了收敛速度太慢，太大了会在极值附近震荡
在梯度平缓的地方下降缓慢，在梯度险峻的地方容易抖动，不是所有的参数的学习率是一样的，比如一些比较稀疏的训练数据，有些特征出现次数少

学习速率调整，or 学习速率调度 learning rate schedules：在每次更新过程中改变学习速率

在损失函数（可能是凸函数）上面梯度下降，找到极值

超参数：学习速率、参数初始值

## 冲量梯度下降算法

在梯度缓慢的地方下降更快，在险峻的地方减少抖动

## NAG 冲量梯度下降算法改进版

## AdaGrad

学习速率自适应，学习速率逐渐衰减，经常更新的学习速率衰减更快
只做了一件事：根据参数来自适应调整学习率，对于

AdaGrad是Duchi在2011年提出的一种学习速率自适应的梯度下下降算法。在训练迭代过程,其学习速率是
逐渐衰减的,经常更新的参数其学习速率衰减更快,这是一种自适度应算法。尽管我们可以根据损失函数的梯度
来加快更新参数,我们也希望能够根据参数的重要性来决定其更新的的幅度。AdaGrad是一种基于梯度算法的优
化算法,它只做了一件事:根据参数来自适应调整学习率。对于不常出现的参数进行较大的更新,对于经常出现
的参数进行较少的更新,因此,这种方法非常适合处理稀疏数据。不在梯度大的维度,减小下降速度;在梯度小
的维度,加快下降速度让学习率适应参数,对于出现次数较少的特征,我们对其采用更大的学习率,对于出现
次数较多的特征,我们对其采用较小的学习率。因此,Adagrad非常常适合处理稀疏数据。Adagrad算法的一个
主要优点是无需手动调整学习率Adagrad的一个主要缺点是它在分母中累加梯度的平方:由于每增加一个正
项,在整个训练过程中,累加的和会持续增长。这会导致学习率变小以至于最终变得无限小,在学习率无限小
时,Adagrad算法将无法取得额外的信息。


## AdaDelta

