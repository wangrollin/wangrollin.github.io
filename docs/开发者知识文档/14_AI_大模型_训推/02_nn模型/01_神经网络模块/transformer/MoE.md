
MoE，全称Mixture of Experts，混合专家模型 https://www.36kr.com/p/2764338482988807

老的思路在大模型上的运用

如何准确的训练出来，还是一个挑战


