闲谈

MLLM https://cloud.tencent.com/developer/article/2322835

模型/训练/推理知识介绍-预训练/微调/对齐/RAG-自回归 https://developer.volcengine.com/articles/7389518187223580710

softmax

向前传播：损失函数、激活函数、神经元模型（计算单元）；反向传播、梯度、梯度消失；学习率；epoch 训练次数

编解码器
解码器

p-tuning 基于提示词的调优 https://developer.volcengine.com/articles/7389518861043859483

AI Agent的核心驱动力是大模型，并在此基础上增加了规划（Planning）、记忆（Memory）和工具使用（Tool Use）三个关键组件。

softmax sigmod


梯度、反向传播：

prompt 工程
- 思维链 Chain of Thought, CoT
- 思维树 Tree of Thought, ToT
- 思维图 Graph of Tree, GoT
- 累计推理
- 基于反馈的 ReACT-Synergizing Reasoning and Acting
- Reflexion

ai agent 最新技术综述 https://developer.volcengine.com/articles/7389832448634257419#heading3

agent 框架
- 单智能体 AutoGPT BabyAGI
- 多智能体开源框架：autoGen、XAgent、AutoAgents


MoE，全称Mixture of Experts，混合专家模型 https://www.36kr.com/p/2764338482988807

微调 https://blog.csdn.net/sinat_39620217/article/details/131751780
- SFT 监督微调（Supervised Fine-tuning）
- LoRA
- p-tuning
- Freeze 监督微调

数据预处理(Tokenizer分词器)、模板（Template）设计以及LLM技术选型 https://cloud.tencent.com/developer/article/2421668

为啥Decoder-Only这条路线效果最好 https://www.cnblogs.com/ghj1976/p/decoderonly.html

RL 强化学习



# 显卡消耗

- 参数大小：1B
  - 模型运行内存：2G （*2
  - LoRA 微调：2.3G （*2.3
  - 全量微调：12G （*12
    - 模型内存 2G
    - 2G
    - 8G

参数大小：8B
模型运行内存：16G
全量微调：96G
LoRA 微调：18.4G

# 大图梳理

特征工程

计算图
参数
损失函数
优化算法（梯度下降）
学习率
反向传播

cnn rnn lstm 各种nn模块

kaggle 打天梯
hugging face 模型乐园

pytorch 训练引擎
ray 分布式引擎
异构显卡训练平台

DL RL NN
nlp -> transformer -> self-attention

预训练
持续预训练
SFT：灾难性遗忘、全量微调、LoRA 微调
对齐 RLHF

onnx: netron
vllm
ollama
推理加速: gpu NPU
流式 grpc
tokenize
上下文长度 128k

autoGen
LangChain
graphRAG

llm: llama 3.1
lmm: 多模态


# 梯度下降

梯度好文 https://blog.csdn.net/iqdutao/article/details/107174240

学习率/步长

## 批量梯度下降算法 BGD
全量梯度下降不能进行在线模型参数更新
更新参数时使用全部样本来进行更新，每次前进都是正确的方向，下降过程基本是直线


## 随机梯度下降算法 SGD

每次更新系数只随机抽取一个样本进行计算，每次更新不一定是正确的方向。随机性比较大，下降过程比较曲折

## 小批量梯度下降 MBGD
每次使用一部分样本，一般小于256，这样比SGD稳定，比BGD快

## GD 下降的困难

学习速率很难选择到一个合理的，太小了收敛速度太慢，太大了会在极值附近震荡
在梯度平缓的地方下降缓慢，在梯度险峻的地方容易抖动，不是所有的参数的学习率是一样的，比如一些比较稀疏的训练数据，有些特征出现次数少

学习速率调整，or 学习速率调度 learning rate schedules：在每次更新过程中改变学习速率

在损失函数（可能是凸函数）上面梯度下降，找到极值

超参数：学习速率、参数初始值

## 冲量梯度下降算法

在梯度缓慢的地方下降更快，在险峻的地方减少抖动

## NAG 冲量梯度下降算法改进版

## AdaGrad

学习速率自适应，学习速率逐渐衰减，经常更新的学习速率衰减更快
只做了一件事：根据参数来自适应调整学习率，对于

AdaGrad是Duchi在2011年提出的一种学习速率自适应的梯度下下降算法。在训练迭代过程,其学习速率是
逐渐衰减的,经常更新的参数其学习速率衰减更快,这是一种自适度应算法。尽管我们可以根据损失函数的梯度
来加快更新参数,我们也希望能够根据参数的重要性来决定其更新的的幅度。AdaGrad是一种基于梯度算法的优
化算法,它只做了一件事:根据参数来自适应调整学习率。对于不常出现的参数进行较大的更新,对于经常出现
的参数进行较少的更新,因此,这种方法非常适合处理稀疏数据。不在梯度大的维度,减小下降速度;在梯度小
的维度,加快下降速度让学习率适应参数,对于出现次数较少的特征,我们对其采用更大的学习率,对于出现
次数较多的特征,我们对其采用较小的学习率。因此,Adagrad非常常适合处理稀疏数据。Adagrad算法的一个
主要优点是无需手动调整学习率Adagrad的一个主要缺点是它在分母中累加梯度的平方:由于每增加一个正
项,在整个训练过程中,累加的和会持续增长。这会导致学习率变小以至于最终变得无限小,在学习率无限小
时,Adagrad算法将无法取得额外的信息。


## AdaDelta

## ？梯度爆炸、梯度消失

https://blog.csdn.net/weixin_46470894/article/details/107145207


# ？损失函数和优化函数（梯度下降）的使用

https://blog.csdn.net/qq_38890412/article/details/109193294

# ？手搓反向传播

https://blog.csdn.net/kkkaiyu/article/details/135184157


# ？cuda tensorRT

# ？xgboost pytorch sklearn 的区别是什么

# ?backbone

# ? ResNet


# d:\wangrollin\wangrollin.github.io-master\docs\开发者知识文档\15_AI应用\01_开发范式\agent.md

https://blog.csdn.net/2401_84204207/article/details/139698041?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-139698041-blog-138760037.235^v43^pc_blog_bottom_relevance_base7&spm=1001.2101.3001.4242.1&utm_relevant_index=3

# d:\wangrollin\wangrollin.github.io-master\docs\开发者知识文档\15_AI应用\01_开发范式\rag.md

https://developer.volcengine.com/articles/7373874019113631754
https://blog.csdn.net/qq_63585949/article/details/135916931

graphRag -- 知识图谱的 rag，在构建知识图谱的时候需要 llm 来构建。目前处于探索阶段，成本较高




闲谈2

## qLoRA，量化 lora 微调，更省资源

## coze 扣子

bot 构建平台

bot（单agent or 多agent）

- 规划能力
- 插件
  - 工具1 - 接口1
  - 工具2 - 接口2
- 记忆
  - 短期记忆
  - 长期记忆
- 知识库
  - 文本
  - 图片
  - 表格


## 知识密度：能力/参数量


## ？向量表征模型 embedding

emdedding模型和 llm 的 tokenize 分词，是不是一个功能，llm在分词后直接返回，是不是就是embedding模型了？


## ？梯度爆炸、梯度消失

https://blog.csdn.net/weixin_46470894/article/details/107145207

# ？ 蒸馏 量化

# ？损失函数和优化函数（梯度下降）的使用

https://blog.csdn.net/qq_38890412/article/details/109193294

# ？手搓反向传播

https://blog.csdn.net/kkkaiyu/article/details/135184157


# ？cuda tensorRT

# ？xgboost pytorch sklearn 的区别是什么

# ？backbone

# ？ResNet

# ？python grpc 流式

原理是什么？


# ？SOFA 是什么

# ?kvCache 在剪枝中的应用



## db 基础字段

id
created_time
updated_time
created_by
updated_by

## yum

yum list available
yum list installed
yum search
yum remove



## 数据湖 hudi upsert和增量处理 原语 的支持，补充了hadoop生态的弱点，每次都要全量执行一个分区

https://www.infoq.cn/article/CAgIDpfJBVcJHKJLSbhe

## lambda 架构包含两层，一层是 速度层，用流给出接近的答案，一层是 批处理层，给出准确的结果。kappa 架构只有速度层

## sr ch 对比区别，适用场景

https://zhuanlan.zhihu.com/p/441059030
https://www.cnblogs.com/syw20170419/p/16421193.html
https://segmentfault.com/a/1190000043343467
https://blog.csdn.net/dan20211/article/details/121711042
https://blog.csdn.net/penriver/article/details/130046033
https://www.infoq.cn/article/DMGT8JSsG5rrxsItVPwv

## 数仓建设规范

https://www.infoq.cn/article/DMGT8JSsG5rrxsItVPwv

## paimon 教程

https://www.cnblogs.com/itxiaoshen/p/17604141.html

## 数据湖的介绍

https://www.oracle.com/cn/big-data/data-lake/what-is-data-lake/

读取更多格式，比如 csv，schemaOnRead
外表
upsert
sql事务

## openfeign

要注意，调用失败会抛出错误，记得处理

**使用方的app：**

@EnableFeignClients({"com.inovance.auto.bigdata.common.web.feignCients"})


**组件库：**

@Component
@FeignClient("bigdata-auth")
public interface AuthClient {

    @GetMapping(value = "/auth/v1/user/email", headers = {"Cookie", "access-token=${service.access.token}"})
    public String getUserEmail(@RequestParam(value = "account") @Size(min = 1, max = 32) String account);
}


## multipartfile 讲解文档

https://blog.csdn.net/Aplumage/article/details/126347647 

大小限制文档 https://blog.csdn.net/tangchao_17/article/details/103223020

## easyexcel 文档

https://blog.csdn.net/sinat_32366329/article/details/103109058


## spring requestParamter date 配置

https://juejin.cn/post/6844904040988409869


## DO LocalDataTime 序列化不带 T

import com.fasterxml.jackson.annotation.JsonFormat;

 @JsonFormat(pattern = "yyyy-MM-dd HH:mm:ss", timezone = "GMT+8")
    @JsonFormat(pattern = "yyyy-MM-dd HH:mm:ss")


## linux 内核和版本

https://www.cnblogs.com/opensmarty/p/10936315.html

