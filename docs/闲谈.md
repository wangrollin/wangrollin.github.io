闲谈

MLLM https://cloud.tencent.com/developer/article/2322835

模型/训练/推理知识介绍-预训练/微调/对齐/RAG-自回归 https://developer.volcengine.com/articles/7389518187223580710

softmax

向前传播：损失函数、激活函数、神经元模型（计算单元）；反向传播、梯度、梯度消失；学习率；epoch 训练次数

编解码器
解码器

p-tuning 基于提示词的调优 https://developer.volcengine.com/articles/7389518861043859483

AI Agent的核心驱动力是大模型，并在此基础上增加了规划（Planning）、记忆（Memory）和工具使用（Tool Use）三个关键组件。

softmax sigmod


梯度、反向传播：

prompt 工程
- 思维链 Chain of Thought, CoT
- 思维树 Tree of Thought, ToT
- 思维图 Graph of Tree, GoT
- 累计推理
- 基于反馈的 ReACT-Synergizing Reasoning and Acting
- Reflexion

ai agent 最新技术综述 https://developer.volcengine.com/articles/7389832448634257419#heading3

agent 框架
- 单智能体 AutoGPT BabyAGI
- 多智能体开源框架：autoGen、XAgent、AutoAgents

微调 https://blog.csdn.net/sinat_39620217/article/details/131751780
- SFT 监督微调（Supervised Fine-tuning）
- LoRA
- p-tuning
- Freeze 监督微调

数据预处理(Tokenizer分词器)、模板（Template）设计以及LLM技术选型 https://cloud.tencent.com/developer/article/2421668

为啥Decoder-Only这条路线效果最好 https://www.cnblogs.com/ghj1976/p/decoderonly.html

RL 强化学习



# 显卡消耗

- 参数大小：1B
  - 模型运行内存：2G （*2
  - LoRA 微调：2.3G （*2.3
  - 全量微调：12G （*12
    - 模型内存 2G
    - 2G
    - 8G

参数大小：8B
模型运行内存：16G
全量微调：96G
LoRA 微调：18.4G

# 大图梳理

特征工程

计算图
参数
损失函数
优化算法（梯度下降）
学习率
反向传播

cnn rnn lstm 各种nn模块

kaggle 打天梯
hugging face 模型乐园

pytorch 训练引擎
ray 分布式引擎
异构显卡训练平台

DL RL NN
nlp -> transformer -> self-attention

推理加速: gpu NPU
流式 grpc
tokenize
上下文长度 128k

llm: llama 3.1
lmm: 多模态


## ？梯度爆炸、梯度消失

https://blog.csdn.net/weixin_46470894/article/details/107145207


# ？损失函数和优化函数（梯度下降）的使用

https://blog.csdn.net/qq_38890412/article/details/109193294

# ？手搓反向传播

https://blog.csdn.net/kkkaiyu/article/details/135184157

# ？cuda tensorRT

# ？xgboost pytorch sklearn 的区别是什么

# ?backbone





## 知识密度：能力/参数量


## ？向量表征模型 embedding

emdedding模型和 llm 的 tokenize 分词，是不是一个功能，llm在分词后直接返回，是不是就是embedding模型了？


## ？梯度爆炸、梯度消失

https://blog.csdn.net/weixin_46470894/article/details/107145207

# ？ 蒸馏 量化

# ？损失函数和优化函数（梯度下降）的使用

https://blog.csdn.net/qq_38890412/article/details/109193294

# ？手搓反向传播

https://blog.csdn.net/kkkaiyu/article/details/135184157


# ？cuda tensorRT

# ？xgboost pytorch sklearn 的区别是什么

# ？backbone

# ？ResNet

# ？python grpc 流式

原理是什么？


# ？SOFA 是什么

# ?kvCache beam search






## db 基础字段

id
created_time
updated_time
created_by
updated_by




## multipartfile 讲解文档

https://blog.csdn.net/Aplumage/article/details/126347647 

大小限制文档 https://blog.csdn.net/tangchao_17/article/details/103223020

## spring requestParamter date 配置

https://juejin.cn/post/6844904040988409869


## DO LocalDataTime 序列化不带 T

import com.fasterxml.jackson.annotation.JsonFormat;

 @JsonFormat(pattern = "yyyy-MM-dd HH:mm:ss", timezone = "GMT+8")
    @JsonFormat(pattern = "yyyy-MM-dd HH:mm:ss")


## linux 内核和版本

https://www.cnblogs.com/opensmarty/p/10936315.html

